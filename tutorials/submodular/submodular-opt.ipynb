{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Monotone submodular maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare algorithms for (constrained) monotone submodular maximization. We will focus on cardinality constraints $\\lvert S \\rvert \\leq k$, i.e. we seek to solve $\\max_{S : \\lvert S \\rvert \\leq k} f(S)$. We evaluate the algorithms on a function $f$ which is concave-over-modular. We implement two such functions: the first, `f_con_easy`, is generated from a modular function with random weights; the second, `f_con`, is initialized similarly, but 5 random elements are chosen to have especially high weight. This may affect the behavior of the algorithms we implement later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concave_over_modular(weights, S):\n",
    "    weights_arr = np.array(weights)\n",
    "    return np.sqrt(np.sum(weights_arr[list(S)]))\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 10000\n",
    "V_con = set(range(n))\n",
    "\n",
    "# version of concave-over-modular with random weights\n",
    "weights = np.random.rand(n)\n",
    "f_con_easy = lambda S: concave_over_modular(weights, S)\n",
    "\n",
    "# version of concave-over-modular with some planted good items\n",
    "weights_planted = weights.copy()\n",
    "num_planted_weights = 5\n",
    "inx_planted_weights = np.random.choice(list(range(len(weights))), size=num_planted_weights, replace=False)\n",
    "weights_planted[inx_planted_weights] = 2 + 2 * np.random.rand(num_planted_weights)\n",
    "f_con = lambda S: concave_over_modular(weights_planted, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithms we consider access $f$ only through its marginal gains $\\Delta(S, i) = f(S\\cup\\{i\\}) - f(S)$. For some functions $f$, the marginal gains can be computed much faster than $f$ itself. For now, we give a helper function to compute $\\Delta(S,i)$ for generic $f$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marg_from_f(f):\n",
    "    def marg(S, i):\n",
    "        return f(S.union([i])) - f(S)\n",
    "    \n",
    "    return marg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create marginal gains functions for both `f_con_easy` and `f_con`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marg_con = marg_from_f(f_con)\n",
    "marg_con_easy = marg_from_f(f_con_easy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to find a near-optimal subset $S$ of size $S \\leq k$, where $k$ is set as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the basic greedy algorithm below. To debug, you can check your implementation against `lazy_greedy` (which we implemented for you) below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">TODO: implement</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(marg, V, k):\n",
    "    # implement greedy:\n",
    "    # marg(S, i) returns f(S \\union i) - f(S)\n",
    "    # V is the ground set\n",
    "    # k is the constraint on |S|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the object value achieved and the runtime for `greedy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "S = greedy(marg_con, V_con, k)\n",
    "print('Took {0:.5f} seconds, objective value {1:.5f}'.format(time.time() - start, f_con(S)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your convenience, lazy greedy is implemented here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def lazy_greedy(marg, V, k):\n",
    "    if k >= len(V):\n",
    "        return V.copy()\n",
    "    \n",
    "    # initialize heap to have (-1 times) upper bound on gain of each item\n",
    "    marg_gains = [marg(set(), i) for i in V]\n",
    "    neg_marg_gains = [-x for x in marg_gains]\n",
    "    neg_marg_gains_tups = list(zip(neg_marg_gains, V))\n",
    "    \n",
    "    heapq.heapify(neg_marg_gains_tups)\n",
    "    \n",
    "    S = set()\n",
    "    \n",
    "    for _ in range(k):\n",
    "        _, i = heapq.heappop(neg_marg_gains_tups)\n",
    "        marg_gain_i = marg(S, i)\n",
    "        \n",
    "        # keep updating the marginal gains and re-inserting\n",
    "        # until the top item of the heap is actually the best\n",
    "        while marg_gain_i < -neg_marg_gains_tups[0][0]:\n",
    "            heapq.heappush(neg_marg_gains_tups, (-marg_gain_i, i))\n",
    "            _, i = heapq.heappop(neg_marg_gains_tups)\n",
    "            marg_gain_i = marg(S, i)\n",
    "            \n",
    "        S.add(i)\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the object value achieved and the runtime for `lazy_greedy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "S = lazy_greedy(marg_con, V_con, k)\n",
    "print('Took {0:.5f} seconds, objective value {1:.5f}'.format(time.time() - start, f_con(S)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement stochastic greedy here. Stochastic greedy takes an additional parameter $s$, the number of candidate elements to sample each round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">TODO: implement</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_greedy(marg, V, k, s):\n",
    "    # implement stochastic greedy:\n",
    "    # marg(S, i) returns f(S \\union i) - f(S)\n",
    "    # V is the ground set\n",
    "    # k is the constraint on |S|\n",
    "    # s is the number of elements to sample each round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the object value achieved and the runtime for `stochastic_greedy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "s = 100\n",
    "S = stochastic_greedy(marg_con, V_con, k, s)\n",
    "print('Took {0:.5f} seconds, objective value {1:.5f}'.format(time.time() - start, f_con(S)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More extensive timing and value comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have working implementations for `greedy`, `lazy_greedy`, and `stochastic_greedy`, run the following code to compare their runtimes and objective values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compare_greedy_algs(f, V, s=100, num_stochastic_trials=10, k_max=10):\n",
    "    # s: number of items that stochastic greedy samples\n",
    "\n",
    "    marg = marg_from_f(f)\n",
    "\n",
    "    times = []\n",
    "    vals = []\n",
    "    labels = []\n",
    "    ks_list = []\n",
    "\n",
    "    ks = list(range(1, k_max+1))\n",
    "    for k in ks:\n",
    "        start = time.time()\n",
    "        S = greedy(marg, V, k)\n",
    "        standard_time = time.time() - start        \n",
    "        \n",
    "        times.append(standard_time)\n",
    "        vals.append(f(S))\n",
    "        labels.append('Standard')\n",
    "        ks_list.append(k)\n",
    "\n",
    "        start = time.time()\n",
    "        S = lazy_greedy(marg, V, k)\n",
    "        lazy_time = time.time() - start\n",
    "\n",
    "        times.append(lazy_time)\n",
    "        vals.append(f(S))\n",
    "        labels.append('Lazy')\n",
    "        ks_list.append(k)\n",
    "\n",
    "        for _ in range(num_stochastic_trials):\n",
    "            start = time.time()\n",
    "            S = stochastic_greedy(marg, V, k, s)\n",
    "            stochastic_time = time.time() - start\n",
    "\n",
    "            times.append(stochastic_time)\n",
    "            vals.append(f(S))\n",
    "            labels.append('Stochastic')\n",
    "            ks_list.append(k)\n",
    "\n",
    "        print('Done with k = {}/{}'.format(k, k_max))\n",
    "        \n",
    "    df = pd.DataFrame({\n",
    "        'time': times,\n",
    "        'val': vals,\n",
    "        'label': labels,\n",
    "        'k': ks_list\n",
    "    })\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out the algorithms on `f_con`, the objective function with planted high-weight elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compare_greedy_algs(f_con, V_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.lineplot(x='k', y='time', hue='label', data=df)\n",
    "plt.title('Runtime of three greedy variants (s); $f_{con}$')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.lineplot(x='k', y='val', hue='label', data=df)\n",
    "plt.title('Objective value of three greedy variants; $f_{con}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test on the easier objective function `f_con_easy` where all weights are sampled uniformly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compare_greedy_algs(f_con_easy, V_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.lineplot(x='k', y='time', hue='label', data=df)\n",
    "plt.title('Runtime of three greedy variants (s); $f_{con\\_easy}$')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.lineplot(x='k', y='val', hue='label', data=df)\n",
    "plt.title('Objective value of three greedy variants; $f_{con\\_easy}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to ponder:\n",
    "- What is the worst-case time complexity of greedy?\n",
    "- When will lazy greedy be much faster than greedy? Can it ever be slower?\n",
    "- How does the solution quality of greedy compare to that of lazy greedy?\n",
    "- How does the solution quality and speed of stochastic greedy change as a function of the number of subsamples $s$?\n",
    "- On what kinds of problems does stochastic greedy attain much worse solution quality than the other two algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Non-monotone submodular maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compare algorithms for non-monotone submodular maximization. \n",
    "\n",
    "First, we focus on the unconstrained case. \n",
    "To motivate this problem, we will take a brief detour and discuss graph cuts. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph cuts\n",
    "The cut function $f(S)$ of a graph measures how strong the connection is between the nodes in $S$ and all other nodes (in $S^C$).\n",
    "Formally, the cut function $f(S)$ is given by $f(S) = \\sum_{i\\in S} \\sum_{j\\in S^C} a_{ij}$; here $a_{ij}$ is the edge weight between nodes $i$ and $j$, or the $ij$-th component of the adjacency matrix $A$ (which you may assume is symmetric). \n",
    "\n",
    "The classic MAX CUT problem can be cast as an unconstrained non-monotone submodular maximization problem $\\max_S f(S)$, where $f(S)$ is the cut function of the graph. \n",
    "\n",
    "Below we implement the cut function for you. First, the following utility code will sample a random graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erdos_renyi(n, p):\n",
    "    A = np.random.binomial(n=1, p=p, size=(n, n))\n",
    "    B = np.triu(A) + np.triu(A).T - np.diag(np.diag(A))\n",
    "    \n",
    "    return B # the adjacency matrix\n",
    "\n",
    "def sbm(n, p, q):\n",
    "    # p is within cluster, q between cluster\n",
    "    # each cluster of size n\n",
    "    \n",
    "    A1 = erdos_renyi(n, p)\n",
    "    A2 = erdos_renyi(n, p)\n",
    "    B = np.random.binomial(n=1, p=q, size=(n,n))\n",
    "    \n",
    "    A_top = np.hstack([A1, B])\n",
    "    A_bottom = np.hstack([B.T, A2])\n",
    "    A = np.vstack([A_top, A_bottom])\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cut(A, S)` returns the value of the cut function when the adjacency matrix of the graph is `A`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(A, S):\n",
    "    n = A.shape[0]\n",
    "    S_C = list(set(range(n)) - S)\n",
    "    return np.sum(A[np.ix_(list(S), S_C)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample a random graph and build its associated cut function. We also define `marg_cut`, the marginal gains function for $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the graph and shuffle its nodes\n",
    "A_clean = sbm(100, 0.05, 0.3)\n",
    "inx = np.random.permutation(A_clean.shape[0])\n",
    "A = A_clean[np.ix_(inx,inx)]\n",
    "\n",
    "# instantiate the cut function for the sampled graph\n",
    "V_cut = set(range(A.shape[0]))\n",
    "f_cut = lambda S: cut(A, S)\n",
    "marg_cut = marg_from_f(f_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we move onto optimization algorithms. The simplest approximation algorithm for unconstrained non-monotone submodular maximization is to return a uniform random set. Implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_selection(V):\n",
    "    V_arr = np.array(list(V))\n",
    "        \n",
    "    to_take = np.random.randint(2, size=len(V), dtype=np.bool)\n",
    "    S_arr = V_arr[to_take]\n",
    "    \n",
    "    return set(S_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the mean object value achieved and the runtime. Random sampling should be fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 50\n",
    "start = time.time()\n",
    "cut_vals_random_selection = [f_cut(random_selection(V_cut)) for _ in range(num_trials)]\n",
    "print('Took {0:.5f} seconds, mean objective value {1:.5f}'.format(time.time() - start, np.mean(cut_vals_random_selection)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic double greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have implemented the deterministic version of the double greedy algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_double_greedy(marg, V):\n",
    "    A = set()\n",
    "    B = V.copy()\n",
    "    \n",
    "    for i in V:\n",
    "        alpha = marg(A, i)\n",
    "        beta = -marg(B.difference([i]), i) # f(B.difference([i])) - f(B)\n",
    "        \n",
    "        if alpha >= beta:\n",
    "            A.add(i)\n",
    "        else:\n",
    "            B.remove(i)\n",
    "            \n",
    "    return A # = B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the objective value achieved and the runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "S = deterministic_double_greedy(marg_cut, V_cut)\n",
    "cut_val_deterministic = f_cut(S)\n",
    "print('Took {0:.5f} seconds, objective value {1:.5f}'.format(time.time() - start, f_cut(S)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized double greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement the randomized version of double greedy. Instead of deterministically updating $A$ when $\\alpha \\geq \\beta$, we update $A$ with probability $\\alpha / (\\alpha + \\beta)$.\n",
    "\n",
    "Depending on your particular implementation, you may have to clip $\\alpha / (\\alpha + \\beta)$ to lie within $[0,1]$. \n",
    "* When and why can this happen?\n",
    "* What does it mean when $\\alpha = -\\beta$?\n",
    "\n",
    "Relate your answers to properties of $f$, e.g. submodularity, monotonicity..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">TODO: implement</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_double_greedy(marg, V):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the object value achieved and the runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 50\n",
    "start = time.time()\n",
    "cut_vals_randomized_double_greedy = [f_cut(randomized_double_greedy(marg_cut, V_cut)) for _ in range(num_trials)]\n",
    "print('Took {0:.5f} seconds, mean objective value {1:.5f}'.format(time.time() - start, np.mean(cut_vals_randomized_double_greedy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these three approachs compare empirically? Remember,\n",
    "* random selection obtains a 1/4-approximation\n",
    "* randomized double greedy obtains a 1/2-approximation (optimal)\n",
    "* deterministic double greedy obtains a 1/3-approximation\n",
    "\n",
    "Below we plot histograms of the objective values achieved by each algorithm. (make sure to run all the code from the above section to populate the arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.hist(cut_vals_random_selection)\n",
    "plt.hist(cut_vals_randomized_double_greedy)\n",
    "plt.stem([cut_val_deterministic], [10], 'r')\n",
    "\n",
    "plt.legend(['Random selection','Randomized double greedy','Deterministic double greedy'])\n",
    "plt.xlabel('Objective value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Objective values $f(S)$ achieved by algorithms for\\nnon-monotone, unconstrained submodular maximization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster marginal gain oracles (time-permitting)\n",
    "Remember, we do not need $f$ itself, but rather the marginal gains $\\Delta(S,i)$. (this holds true also for many of the approaches to submodular _minimization_). For cut functions, it is possible to compute $\\Delta(S,i)$ much faster than simply evaluating $f(S\\cup\\{i\\})$ and $f(S)$. Below, give a faster implementation of $\\Delta(S,i)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">TODO: implement (time-permitting)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_marg_cut_from_A(A):\n",
    "\n",
    "    def marg_cut_fast(S, i):\n",
    "        # your code here; given an adjacency matrix A, a set S, and an element i not in S,\n",
    "        # compute the marginal gains of the cut function \n",
    "\n",
    "    return marg_cut_fast\n",
    "    \n",
    "marg_cut_fast = build_marg_cut_from_A(A)\n",
    "    \n",
    "# sanity check -- these two marginal gains oracles should always return the same value\n",
    "print(marg_cut_fast(set([1,2,3]), 4))\n",
    "print(marg_cut(set([1,2,3]), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we include code so you can compare the speed of the two implementations of $\\Delta(S,i)$ as the size of the underlying graph changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_of_n_nodes(n):\n",
    "    # sample the graph and shuffle its nodes\n",
    "    n_half = int(n / 2)\n",
    "    A_clean = sbm(n_half, 0.05, 0.3)\n",
    "    inx = np.random.permutation(A_clean.shape[0])\n",
    "    A = A_clean[np.ix_(inx,inx)]\n",
    "    \n",
    "    return A\n",
    "\n",
    "\n",
    "def test_oracles_different_sized_graphs(n_list, n_trials=10):\n",
    "    import time\n",
    "    \n",
    "    times_slow = []\n",
    "    times_fast = []\n",
    "    \n",
    "    for n in n_list:\n",
    "        A = graph_of_n_nodes(n)\n",
    "        \n",
    "        V = list(range(n-1)) # note we exclude the final node\n",
    "        S_list = [random_selection(V) for _ in range(n_trials)]\n",
    "        i = n - 1 # does not appear in any S above\n",
    "        \n",
    "        f_cut = lambda S: cut(A, S)\n",
    "        marg_cut = marg_from_f(f_cut)\n",
    "        marg_cut_fast = build_marg_cut_from_A(A)\n",
    "\n",
    "        start = time.time()\n",
    "        all_vals_slow = [marg_cut(S, i) for S in S_list]\n",
    "        time_slow = time.time() - start\n",
    "        times_slow.append(time_slow / n_trials)\n",
    "        \n",
    "        start = time.time()\n",
    "        all_vals_fast = [marg_cut_fast(S, i) for S in S_list]\n",
    "        time_fast = time.time() - start\n",
    "        times_fast.append(time_fast / n_trials)\n",
    "        \n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(n_list, times_slow)\n",
    "    plt.plot(n_list, times_fast)\n",
    "    plt.legend(['Naive $\\Delta(S,i)$', 'Fast $\\Delta(S,i)$'])\n",
    "    plt.xlabel('Number of nodes in graph')\n",
    "    plt.ylabel('Average evaluation time (s)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_list = list(range(10, 501, 30))\n",
    "test_oracles_different_sized_graphs(n_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here we focus on an image summarization task. We ingest a dataset (in this case MNIST), compute a similarity matrix between all datapoints, and use that to test a number of different formulations of summarizing the dataset.\n",
    "\n",
    "We preprocess the dataset `X` by computing HOG features. Feel free to explore other features from `skimage`, or using e.g. raw pixel features. (Note: if you do this, you may need to adjust the length scale of the kernel below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "from sklearn.datasets import fetch_openml\n",
    "X_raw, y_full = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "print('Loading MNIST took {} seconds'.format(time.time() - start))\n",
    "\n",
    "# efficient feature map for images\n",
    "def hog_features(X_raw):\n",
    "    from skimage.feature import hog\n",
    "\n",
    "    hog_feat = [hog(np.reshape(X_raw[i,:], (28, 28))) for i in range(X_raw.shape[0])]\n",
    "    X = np.array(hog_feat)\n",
    "    \n",
    "    return X\n",
    "\n",
    "start = time.time()\n",
    "X_full = hog_features(X_raw)\n",
    "print('Feature extraction took {} seconds'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seek a set $S$ of up to $k$ datapoints that well represent the entire dataset. There are a number of ways to frame this as maximizing $f(S)$ for various $f$. Suppose we have access to some similarity score $s(i,j)$ between the $i$-th and $j$-th datapoints, for example, the corresponding entry of a kernel matrix. Some possibilities of $f$ (in terms of $s$) are:\n",
    "* Facility location\n",
    "    - $f(S) = \\sum_{i\\in V} \\max_{j \\in S} s(i,j)$ \n",
    "    - montone submodular; nonnegative\n",
    "* Sum coverage: \n",
    "    - $f(S) = \\sum_{i\\in V} \\sum_{j\\in S} s(i,j)$\n",
    "    - monotone submodular; nonnegative\n",
    "* Facility location with diversity penalty: \n",
    "    - $f(S) = \\sum_{i\\in V} \\max_{j\\in S} s(i,j) - \\frac{1}{|V|} \\sum_{i\\in S} \\sum_{j\\in S} s(i,j)$\n",
    "    - non-monotone submodular; nonnegative\n",
    "* Log-subdeterminants:\n",
    "    - $f(S) = \\log\\det K_{S,S}$\n",
    "    - non-monotone submodular; may be negative\n",
    "    \n",
    "Explore these $f$ and other formulations you think up. Play around with the dataset, e.g. test different features of the images, or try out different levels of class imbalance.\n",
    "\n",
    "Keep in mind the following questions:\n",
    "* what desiderata does $f$ encode?\n",
    "* what worst-case guarantees are there w.r.t. optimizing $f$?\n",
    "* regardless of worst-case guarantees, do simple algorithms work well for $f$?\n",
    "* how easy is $f$ to compute?\n",
    "* how robust is $f$ to class imbalance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the objective function $f(S)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of explicitly forming the kernel matrix, we compute similarity values only as needed using m = RBF()\n",
    "\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "m = RBF(length_scale=1e0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define the objective functions discussed above. Each function, e.g. `facloc`, takes in the data matrix `X` and the kernel function `m`, and returns a function oracle `f(S)` that can be optimized using the algorithms above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facility location\n",
    "def facloc(X, m):\n",
    "    \n",
    "    # for efficiency, we subsample V in the outer sum;\n",
    "    # we fix the subsampling ahead of time so it is consistent across oracle calls\n",
    "    n_sample = 1000\n",
    "    n_sample = min(X.shape[0], n_sample)\n",
    "    inx = np.random.choice(list(range(X.shape[0])), size=n_sample, replace=False)\n",
    "    \n",
    "    def f(S):\n",
    "        if len(S) == 0:\n",
    "            return 0\n",
    "        \n",
    "#         # if we weren't subsampling:\n",
    "#         K_S = m(X[list(S),:], X)\n",
    "        K_S = m(X[list(S),:], X[inx,:])\n",
    "        return np.sum(np.max(K_S, axis=0))\n",
    "    \n",
    "    return f\n",
    "\n",
    "# Sum coverage\n",
    "def sum_coverage(X, m):\n",
    "\n",
    "    # for efficiency, we subsample V in the outer sum;\n",
    "    # we fix the subsampling ahead of time so it is consistent across oracle calls\n",
    "    n_sample = 1000\n",
    "    n_sample = min(X.shape[0], n_sample)\n",
    "    inx = np.random.choice(list(range(X.shape[0])), size=n_sample, replace=False)\n",
    "    \n",
    "    def f(S):\n",
    "        if len(S) == 0:\n",
    "            return 0\n",
    "        \n",
    "        K_S = m(X[list(S),:], X[inx,:])\n",
    "        return np.sum(K_S)\n",
    "    \n",
    "    return f\n",
    "\n",
    "# Facility location with diversity penalty\n",
    "def diverse_facloc(X, m):\n",
    "    f_facloc = facloc(X, m)\n",
    "\n",
    "    def f_diverse(S):\n",
    "        if len(S) == 0:\n",
    "            return 0\n",
    "        \n",
    "        K_SS = m(X[list(S),:])\n",
    "        return -np.sum(K_SS) / X.shape[0]\n",
    "    \n",
    "    return lambda S: f_facloc(S) + f_diverse(S)\n",
    "\n",
    "# Log determinant maximization\n",
    "def dpp(X, m):\n",
    "    def f(S):\n",
    "        K_SS = m(X[list(S),:])\n",
    "        (sign, logdet) = np.linalg.slogdet(K_SS)\n",
    "        return logdet\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soon, we will optimize $f(S)$ in order to summarize the dataset. First, we provide some simple code for visualizing a candidate summary $S$ of images returned by the algorithm.\n",
    "\n",
    "We print the labels that correspond to the selected images, and show the chosen images. Consider the following questions when evaluating the quality of the summary $S$:\n",
    "* What is the distribution of labels in the summary? Does it match the overall dataset? \n",
    "* Do the images look typical? Do they look like outliers? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_summary(X_raw, y, S):\n",
    "    # X_raw contains the raw images (not their features)\n",
    "    \n",
    "    S_list = np.array(list(S))\n",
    "    inx_sort = np.argsort(y[S_list])\n",
    "    S_sort = S_list[inx_sort]\n",
    "\n",
    "    labels = ', '.join([str(int(x)) for x in y[S_sort]])\n",
    "    print('Labels: {}'.format(labels))\n",
    "    \n",
    "    show_images = True\n",
    "    if show_images:\n",
    "        height = int(np.ceil(np.sqrt(len(S))))\n",
    "        width = int(np.ceil(len(S) / height))\n",
    "        \n",
    "        fig, axs = plt.subplots(height, width, sharex=True, sharey=True, figsize=(2*height, 2*width))\n",
    "        for c, i in enumerate(S_sort):\n",
    "            X_i = np.reshape(X_raw[i,:], (28, 28))\n",
    "            \n",
    "            \n",
    "            plt.subplot(width, height, c+1)\n",
    "            plt.imshow(X_i)\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding a good summary\n",
    "We encourage you to try different objective functions $f$ and different optimization algorithms, e.g.\n",
    "* `lazy_greedy` (is it still fast enough?)\n",
    "* `stochastic_greedy` with varying number of subsamples $s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the problem (ground set V, constraint k, objective f, marginal gains function)\n",
    "\n",
    "V = set(range(X_full.shape[0]))\n",
    "k = 10\n",
    "\n",
    "# X_full is the full dataset of features; m is the kernel function\n",
    "f = facloc(X_full, m)\n",
    "marg = marg_from_f(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "S = stochastic_greedy(marg, V, k, 100)\n",
    "# S = lazy_greedy(marg, V, k)\n",
    "print('Took {0:.5f} seconds, objective value {1:.5f}'.format(time.time() - start, f(S)))\n",
    "# S = repeated_greedy(f, V, k, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_summary(X_raw, y_full, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of class imbalance\n",
    "Different objective functions $f(S)$ yield summaries that are affected by class imbalance in different ways. Below we provide some code to help you subsample the dataset to have a desired level of class imbalance. (Note: this may also be useful to subsample, in case optimization is taking too long on the full dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to subsample with specified weights for each class\n",
    "def stratified_sampling(X_full, X_raw, y_full, class_weights, n_subsample):\n",
    "    import numpy as np\n",
    "    class_weights = np.array(class_weights) / np.sum(np.array(class_weights))\n",
    "\n",
    "    class_nums = np.round(class_weights * n_subsample).astype(np.int32)\n",
    "    all_class_labels = sorted(set(y_full))\n",
    "\n",
    "    X_sampled_list = []\n",
    "    X_raw_sampled_list = []\n",
    "    y_sampled_list = []\n",
    "    for label, count in zip(all_class_labels, class_nums):\n",
    "        matching_label = (y_full == label)\n",
    "        inxs_this_label = np.random.choice(np.where(matching_label)[0], size=count, replace=False)\n",
    "        X_this_label = X_full[inxs_this_label, :]\n",
    "        X_raw_this_label = X_raw[inxs_this_label, :]\n",
    "        y_this_label = y_full[inxs_this_label]\n",
    "\n",
    "        X_sampled_list.append(X_this_label)\n",
    "        X_raw_sampled_list.append(X_raw_this_label)\n",
    "        y_sampled_list.append(y_this_label)\n",
    "\n",
    "    X_sampled = np.vstack(X_sampled_list)\n",
    "    X_raw_sampled = np.vstack(X_raw_sampled_list)\n",
    "    y_sampled = np.hstack(y_sampled_list)\n",
    "\n",
    "    inxs_permuted = np.random.permutation(len(y_sampled))\n",
    "\n",
    "    return X_sampled[inxs_permuted, :], X_raw_sampled[inxs_permuted, :], y_sampled[inxs_permuted]\n",
    "\n",
    "\n",
    "# method to visualize the specified weights for each class\n",
    "def visualize_class_weights(class_weights_imbalanced):\n",
    "    df = pd.DataFrame({\n",
    "        'label': list(range(len(class_weights_imbalanced))),\n",
    "        'class_weights': class_weights_imbalanced\n",
    "    })\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x='label', y='class_weights', data=df)\n",
    "    plt.xlabel('Class label')\n",
    "    plt.ylabel('Relative frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample so that each class is equally represented\n",
    "\n",
    "n_subsample = 1000\n",
    "\n",
    "class_weights_even = [0.1] * 10\n",
    "X_balanced, X_raw_balanced, y_balanced = stratified_sampling(X_full, X_raw, y_full, class_weights_even, n_subsample)\n",
    "\n",
    "# visualize the class weights\n",
    "visualize_class_weights(class_weights_even)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample so that classes are imbalanced\n",
    "\n",
    "n_subsample = 1000\n",
    "\n",
    "class_weights_imbalanced = [0.2, 0.1, 0.05, 0.05, 0.01, 0.01, 0.02, 0.01, 0.35, 0.2]\n",
    "X_imbalanced, X_raw_imbalanced, y_imbalanced = stratified_sampling(X_full, X_raw, y_full, class_weights_imbalanced, n_subsample)\n",
    "\n",
    "# visualize the class weights\n",
    "visualize_class_weights(class_weights_imbalanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the ground set and constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = set(range(X_imbalanced.shape[0]))\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try lots of objective functions on the imbalanced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for objective in [facloc, diverse_facloc, sum_coverage, dpp]:\n",
    "    print('Results for objective: {}'.format(objective.__name__))\n",
    "    \n",
    "    f = objective(X_imbalanced, m)\n",
    "    marg = marg_from_f(f)\n",
    "\n",
    "    start = time.time()\n",
    "    # S = stochastic_greedy(marg, V, k, 100)\n",
    "    S = lazy_greedy(marg, V, k)\n",
    "    print('Took {0:.5f} seconds, objective value {1:.5f}'.format(time.time() - start, f(S)))\n",
    "    show_summary(X_raw_imbalanced, y_imbalanced, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Repeated greedy\n",
    "Several of the objective functions above are _not_ monotone (in particular, `diverse_facloc` and `dpp`). We wish to optimize these non-monotone submodular functions subject to a cardinality constraint, i.e. $\\max_{S : |S|\\leq k} f(S)$. So far, though, we have only discussed unconstrained non-monotone submodular maximization.\n",
    "\n",
    "There are a handful of algorithms in the literature with guarantees for this problem setting. One of them is called \"repeated greedy,\" which we have implemented for you below. The basic idea is to leverage both the strengths of standard greedy (for dealing with the constraint) with the strengths of double greedy (for dealing with non-monotonicity). The algorithm proceeds for several rounds; each round, \n",
    "* we first use greedy to select $k$ good items;\n",
    "* then use double greedy to select the best subset of those (possibly all of them);\n",
    "* finally, remove the chosen items from consideration and repeat.\n",
    "\n",
    "At the end, we return the best subset chosen over all the rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeated_greedy(f, V, k, s=5):\n",
    "    N = V.copy()\n",
    "    \n",
    "    S_list = []\n",
    "    S_prime_list = []\n",
    "    \n",
    "    marg = marg_from_f(f)\n",
    "    \n",
    "    for i in range(s):\n",
    "        S = lazy_greedy(marg, N, k)\n",
    "        S_prime = deterministic_double_greedy(marg, S)\n",
    "        \n",
    "        S_list.append(S)\n",
    "        S_prime_list.append(S_prime)\n",
    "        \n",
    "        N.difference_update(S)\n",
    "        \n",
    "    S_vals = [(f(S), S) for S in S_list + S_prime_list]\n",
    "    S_vals_sorted = sorted(S_vals, reverse=True)\n",
    "    best_tup = S_vals_sorted[0]\n",
    "    \n",
    "    return best_tup[1]\n",
    "\n",
    "start = time.time()\n",
    "repeated_greedy(f_con, V_con, 10, 5)\n",
    "print('Took {0:.5f} seconds, objective value {1:.5f}'.format(time.time() - start, f_con(S)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the results achieved by `repeated_greedy` versus e.g. `lazy_greedy` for each of these non-monotone objectives. \n",
    "* Which algorithm achieves larger objective value? (is there any difference? why or why not?)\n",
    "* Which algorithm yields a better summary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for objective in [diverse_facloc, dpp]:\n",
    "    print('Results for objective: {}'.format(objective.__name__))\n",
    "    \n",
    "    f = objective(X_imbalanced, m)\n",
    "    \n",
    "    start = time.time()\n",
    "    S = repeated_greedy(f, V, k)\n",
    "    print('Repeated greedy took {0:.5f} seconds, objective value {1:.5f}'.format(time.time() - start, f(S)))\n",
    "    \n",
    "    show_summary(X_raw_imbalanced, y_imbalanced, S)\n",
    "\n",
    "    marg = marg_from_f(f)\n",
    "    start = time.time()\n",
    "    S = lazy_greedy(marg, V, k)\n",
    "    print('In contrast, lazy greedy took {0:.5f} seconds, objective value {1:.5f}'.format(time.time() - start, f(S)))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for objective in [diverse_facloc, dpp]:\n",
    "    print('Results for objective: {}'.format(objective.__name__))\n",
    "    \n",
    "    f = objective(X_balanced, m)\n",
    "    \n",
    "    start = time.time()\n",
    "    S = repeated_greedy(f, V, k)\n",
    "    print('Repeated greedy took {0:.5f} seconds, objective value {1:.5f}'.format(time.time() - start, f(S)))\n",
    "    \n",
    "    show_summary(X_raw_balanced, y_balanced, S)\n",
    "\n",
    "    marg = marg_from_f(f)\n",
    "    start = time.time()\n",
    "    S = lazy_greedy(marg, V, k)\n",
    "    print('In contrast, lazy greedy took {0:.5f} seconds, objective value {1:.5f}'.format(time.time() - start, f(S)))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlss_submod)",
   "language": "python",
   "name": "mlss_submod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
